{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 6242 04 Build Annoy Index On Disk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " import sys\n",
        " sys.version"
      ],
      "metadata": {
        "id": "V36I__KLCep3",
        "outputId": "28be4a3a-8e0b-4dd2-957d-7369d7542a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.7.13 (default, Mar 16 2022, 17:37:17) \\n[GCC 7.5.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZyW-7iUq0tQ",
        "outputId": "77785eb8-cfdf-40e7-96be-1e6a968247ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmJMe48l4VKW",
        "outputId": "efd55d52-6a08-4bcb-ae79-38d1c1fa1a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import requests\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "from transformers import (\n",
        "    ViTForImageClassification, ViTModel, ViTFeatureExtractor,\n",
        "    AutoModelForImageClassification, BeitFeatureExtractor, ViTMAEModel,\n",
        "    DeiTFeatureExtractor, DeiTModel, DetrFeatureExtractor, DetrForSegmentation,\n",
        "    AutoFeatureExtractor, ViTMAEForPreTraining,\n",
        "    DeiTForImageClassificationWithTeacher, ImageGPTForCausalImageModeling,\n",
        "    ImageGPTFeatureExtractor, ImageGPTForImageClassification,\n",
        ")\n",
        "\n",
        "from PIL import Image\n",
        "from annoy import AnnoyIndex\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from collections import defaultdict\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "yHrviPJMq0vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/gdrive')\n",
        "!ls '/gdrive/My Drive/cse6242_project/Data'\n",
        "!unzip -q '/gdrive/My Drive/cse6242_project/Data/celeba'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RIlJs-3xBr7",
        "outputId": "49f1b802-6688-4518-dddd-337f7d56df22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "accuracy_vs_num_images_18k.csv\tceleba.zip  identity_CelebA.txt\n",
            "replace img_align_celeba/img_align_celeba/000001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    # adds a forward layer to the backbone\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "def init_model(identifier):\n",
        "    print(identifier)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if identifier == 'microsoft/beit-base-patch16-224-pt22k-ft22k':\n",
        "        feature_extractor = BeitFeatureExtractor.from_pretrained(identifier)\n",
        "        model = AutoModelForImageClassification.from_pretrained(identifier)\n",
        "        dim = 768\n",
        "    elif identifier == 'google/vit-base-patch16-224':\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained(identifier)\n",
        "        model = ViTForImageClassification.from_pretrained(identifier)\n",
        "        dim = 768\n",
        "    elif identifier == 'facebook/vit-mae-base':\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(identifier)\n",
        "        model = ViTMAEForPreTraining.from_pretrained(identifier)\n",
        "        dim = 196\n",
        "    elif identifier == 'facebook/deit-base-distilled-patch16-224':\n",
        "        feature_extractor = DeiTFeatureExtractor.from_pretrained(identifier)\n",
        "        model = DeiTForImageClassificationWithTeacher.from_pretrained(identifier)\n",
        "        dim = 1000\n",
        "    elif identifier == 'facebook/detr-resnet-50-panoptic':\n",
        "        feature_extractor = DetrFeatureExtractor.from_pretrained(identifier)\n",
        "        model = DetrForSegmentation.from_pretrained(identifier)\n",
        "        dim = 768\n",
        "    elif identifier == 'openai/imagegpt-small':\n",
        "        feature_extractor = ImageGPTFeatureExtractor.from_pretrained(identifier)\n",
        "        model = ImageGPTForImageClassification.from_pretrained(identifier) \n",
        "        dim = 2\n",
        "    elif identifier == 'openai/imagegpt-large':\n",
        "        feature_extractor = ImageGPTFeatureExtractor.from_pretrained(identifier)\n",
        "        model = ImageGPTForCausalImageModeling.from_pretrained(identifier) \n",
        "        dim = 2\n",
        "\n",
        "    model.classifier = Identity()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    return model, feature_extractor, dim\n",
        "\n",
        "def prepare_dataset(n_identities=None):\n",
        "    print('loading data...')\n",
        "    identities = pd.read_csv(\"/gdrive/My Drive/cse6242_project/Data/identity_CelebA.txt\", sep=\" \", header=None)\n",
        "    identities.rename(columns={0: \"file\", 1: \"identity\"}, inplace=True) \n",
        "\n",
        "    if n_identities is not None:\n",
        "        identity_selection = np.random.choice(identities.identity.unique(), n_identities)\n",
        "    else:\n",
        "        identity_selection = identities.identity.unique()\n",
        "\n",
        "    df = identities[identities.identity.isin(identity_selection)].reset_index()\n",
        "    return df, identity_selection\n",
        "\n",
        "def build_annoy_index(model, feature_extractor, df, dim=768, batch_size=25):\n",
        "    print('building annoy index...')\n",
        "    idx_to_identity = df.to_dict('index')\n",
        "    identity_to_idx = defaultdict(list) \n",
        "    index = AnnoyIndex(dim, 'euclidean')\n",
        "    index.on_disk_build(\"on_disk_index.ann\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    image_paths, keys = [], []\n",
        "    for k, v in idx_to_identity.items():\n",
        "        identity_to_idx[v[\"identity\"]].append(k)\n",
        "        image_paths.append(\"img_align_celeba/img_align_celeba/\" + v[\"file\"])\n",
        "        keys.append(k)\n",
        "\n",
        "    # batched computation\n",
        "    print('num images:', len(image_paths))\n",
        "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
        "\n",
        "        images = []\n",
        "        for path in image_paths[i:i+batch_size]:\n",
        "            images.append(Image.open(path))\n",
        "        batch_im, batch_keys = images, keys[i:i+batch_size]\n",
        "        batch_encodings = feature_extractor(images=batch_im, return_tensors=\"pt\")\n",
        "        batch_pixel_values = batch_encodings['pixel_values'].to(device)\n",
        "        batch_outputs = model(batch_pixel_values).logits\n",
        "        for i, embedding in enumerate(batch_outputs):\n",
        "            index.add_item(batch_keys[i], embedding.squeeze())\n",
        "\n",
        "    index.build(128)\n",
        "    index.save('celeba.ann')\n",
        "\n",
        "    return index, idx_to_identity, identity_to_idx\n",
        "\n",
        "def retrival_accuracy(base_idx, image_idx, idx_to_identity, identity_to_idx):\n",
        "    true_identity = idx_to_identity[base_idx][\"identity\"]\n",
        "    n_true_matches = len(identity_to_idx[true_identity])\n",
        "\n",
        "    hits = []\n",
        "\n",
        "    for idx in tqdm(image_idx):\n",
        "        idx_identity = idx_to_identity[idx][\"identity\"]\n",
        "\n",
        "        if idx_identity == true_identity:\n",
        "            hits.append(True)\n",
        "        else:\n",
        "            hits.append(False)\n",
        "\n",
        "    return sum(hits) / min(len(image_idx), n_true_matches)"
      ],
      "metadata": {
        "id": "rVsSWx7Sq0xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_identifier = 'microsoft/beit-base-patch16-224-pt22k-ft22k'\n",
        "\n",
        "results = defaultdict(list)\n",
        "df, identity_selection = prepare_dataset()\n",
        "\n",
        "\n",
        "model, feature_extractor, dim = init_model(model_identifier)\n",
        "index, idx_to_identity, identity_to_idx = build_annoy_index(model, feature_extractor, df, \n",
        "                                                            dim, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YtIM_uzq00F",
        "outputId": "3ff816eb-c230-4902-ab96-a34bbc884a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data...\n",
            "microsoft/beit-base-patch16-224-pt22k-ft22k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building annoy index...\n",
            "num images: 202599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6332/6332 [1:24:10<00:00,  1.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MGUOhfk-q1M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp on_disk_index.ann \"/gdrive/My Drive/cse6242_project/Data\""
      ],
      "metadata": {
        "id": "kn_FvULxVbe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f9ld4Vfi0ZZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}